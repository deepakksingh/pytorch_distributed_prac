{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Credit: https://leimao.github.io/blog/PyTorch-Distributed-Training/\n",
    "'''\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seeds(random_seed=0):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.backends.cudnn.benchmark=False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, device, test_loader):\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(images)\n",
    "            _,predicted = torch.max(outputs.data,1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    num_epochs_default = 10000\n",
    "    batch_size_default = 256\n",
    "    learning_rate_default = 0.1\n",
    "    random_seed_default = 0\n",
    "    model_dir_default = \"saved_models\"\n",
    "    model_filename_default = \"resnet_distributed.pth\"\n",
    "\n",
    "\n",
    "    #each process runs on 1GPU device specified by the local_rank argument\n",
    "    parser = argparse.ArgumentParser(formatter_class = argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument('--local_rank',type=int,help='Local rank. Necessary for using the torch.distributed.launch utility')\n",
    "    parser.add_argument('--num_epochs',type=int, help='Number of training epochs',default=num_epochs_default)\n",
    "    parser.add_argument('--batch_size', type=int, help='Training batch size for one process',default = batch_size_default)\n",
    "    parser.add_argument(\"--learning_rate\", type=float, help=\"Learning rate.\", default=learning_rate_default)\n",
    "    parser.add_argument(\"--random_seed\", type=int, help=\"Random seed.\", default=random_seed_default)\n",
    "    parser.add_argument(\"--model_dir\", type=str, help=\"Directory for saving models.\", default=model_dir_default)\n",
    "    parser.add_argument(\"--model_filename\", type=str, help=\"Model filename.\", default=model_filename_default)\n",
    "    parser.add_argument('--resume', action='store_true',help='Resume training from saved checkpoint.')\n",
    "    argv = parser.parse_args()\n",
    "    \n",
    "    local_rank = argv.local_rank\n",
    "    num_epochs = argv.num_epochs\n",
    "    batch_size = argv.batch_size\n",
    "    learning_rate = argv.learning_rate\n",
    "    random_seed = argv.random_seed\n",
    "    model_dir = argv.model_dir\n",
    "    model_filename = argv.model_filename\n",
    "    resume = argv.resume\n",
    "    \n",
    "    '''\n",
    "    #We should create the directories outside the pytorch program.\n",
    "    Since it is not multiprocess safe\n",
    "    So avoid the following:\n",
    "    \n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    '''\n",
    "    \n",
    "    model_file_path = os.path.join(model_dir, model_filename)\n",
    "    \n",
    "    #we neeed to use seeds to make sure that the models initialized in different processes are the same\n",
    "    set_random_seeds(random_seed)\n",
    "    \n",
    "    #Initializes the distributed backend which will take care of synchronizing nodes/GPUs\n",
    "    torch.distributed.init_process_group(backend='nccl')\n",
    "#     torch.distributed.init_process_group(backend='gloo')\n",
    "    #NOTE: check what is different between nccl and gloo backend\n",
    "    \n",
    "    #Encapsulate the model on the GPU assigned to the current process\n",
    "    model = torchvision.models.resnet18(pretrained=False)\n",
    "    device = torch.device(f'cuda:{local_rank}')\n",
    "    print(device)\n",
    "    model = model.to(device)\n",
    "    ddp_model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank)\n",
    "    \n",
    "    \n",
    "    #We only save the model who uses device 'cuda:0'\n",
    "    #To resume, the device for the saved model would also be 'cuda:0'\n",
    "    if resume==True:\n",
    "        map_location = {'cuda:0':f'cuda:{local_rank}'}\n",
    "        ddp_model.load_state_dict(torch.load(model_filepath, map_location=map_location))\n",
    "        \n",
    "    \n",
    "    #Prepare the dataset and the dataloader\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    \n",
    "    #Data should be prefetched\n",
    "    #Download should be set to False, because it is not multiprocess safe\n",
    "    train_set = torchvision.datasets.CIFAR10(root='data', train=True, download=False, transform=transform)\n",
    "    test_set = torchvision.datasets.CIFAR10(root='data', train=False, download=False, transform=transform)\n",
    "    \n",
    "    #Restricts dataloading to a subset of the dataset exclusive to the current process\n",
    "    train_sampler = DistributedSampler(dataset=train_set)\n",
    "    \n",
    "    train_loader = DataLoader(dataset=train_set, batch_size=batch_size, sampler=train_sampler, num_workers=8)\n",
    "    \n",
    "    #Test loader does not have to follow distributed sampling strategy\n",
    "    test_loader = DataLoader(dataset=test_set, batch_size=128,shuffle=False, num_workers=8)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(ddp_model.parameters(), lr = learning_rate, momentum=0.9, weight_decay = 1e-5)\n",
    "    \n",
    "    \n",
    "    #Loop over the dataset multiple times\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Local Rank: {local_rank}, Epoch: {epoch}, Training...')\n",
    "        \n",
    "        #Save and evaluate model routinely\n",
    "        if epoch %1 == 0:\n",
    "            if local_rank == 0:\n",
    "                accuracy = evaluate(model=ddp_model, device=device, test_loader=test_loader)\n",
    "                torch.save(ddp_model.state_dict(), model_file_path)\n",
    "                print('-'*75)\n",
    "                print(f'Epoch: {epoch}, Accuracy: {accuracy}')\n",
    "                print('-'*75)\n",
    "                \n",
    "    \n",
    "    ddp_model.train()\n",
    "    \n",
    "    for data in train_loader:\n",
    "        inputs, targets = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = ddp_model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a46b7a88e0f4d409f98ef4d8b852193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/cifar-10-python.tar.gz to data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Prepare the dataset and the dataloader\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "    \n",
    "train_set = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
    "test_set = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-dd0eaa9e5b80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_dir' is not defined"
     ]
    }
   ],
   "source": [
    "model_file_path = os.path.join(model_dir, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
